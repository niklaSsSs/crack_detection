# ğŸ—ï¸ SDNET2018 Crack Encoder Pre-training

Pre-training eines EfficientNet-B0 Encoders auf 56K Rissbildern fÃ¼r Transfer Learning auf SchweiÃŸnaht-Segmentierung.

**Ziel:** ROC-AUC â‰¥ 0.95 auf Test Set

---

## ğŸš€ Schnellstart

### Mac Training

```bash
# Dependencies installieren
pip install -r requirements.txt

# Training starten (5-10 Stunden)
./scripts/train_mac.sh
```

### Server Training (NVIDIA A40)

```bash
# Dependencies installieren
pip install -r requirements.txt

# Training starten (2-4 Stunden)
./scripts/train_server.sh
```

---

## ğŸ“Š Dataset

**SDNET2018**
- 56,092 Bilder (256Ã—256 RGB)
- 3 DomÃ¤nen: Bridge, Pavement, Wall
- 2 Klassen: Crack (15.1%), NonCrack (84.9%)
- 0 korrupte Dateien

Details: `data_audit.md`

---

## âš™ï¸ Konfigurationen

### Mac Config (`configs/config_mac.yaml`)

| Parameter | Wert |
|-----------|------|
| Device | CPU |
| Input Size | 256px |
| Batch Size | 64 |
| Epochs | 20 |
| Est. Time | 5-10h |

### Server Config (`configs/config_server_a40.yaml`)

| Parameter | Wert |
|-----------|------|
| Device | CUDA |
| Input Size | 512px |
| Batch Size | 32 |
| Epochs | 30 |
| GPU VRAM | ~18GB (40% von 46GB) |
| GPU Util | 40-60% |
| Est. Time | 2-4h |

**Server GPU Auslastung:**
- Ziel: ~40% VRAM (18GB von 46GB)
- Batch Size 32 = ~18GB VRAM
- Batch Size 24 = ~13GB VRAM (30%)
- Batch Size 48 = ~27GB VRAM (60%)

---

## ğŸ“ Projekt-Struktur

```
crack_model/
â”œâ”€â”€ configs/                # Training Configs
â”‚   â”œâ”€â”€ config_mac.yaml
â”‚   â””â”€â”€ config_server_a40.yaml
â”‚
â”œâ”€â”€ src/                    # Source Code
â”‚   â”œâ”€â”€ train.py           # Main Training Script
â”‚   â”œâ”€â”€ dataset.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ scripts/                # Shell Scripts
â”‚   â”œâ”€â”€ train_mac.sh
â”‚   â””â”€â”€ train_server.sh
â”‚
â”œâ”€â”€ outputs/                # Training Outputs
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ checkpoints/       # â­ encoder_weights.pt hier!
â”‚   â””â”€â”€ visualizations/
â”‚
â””â”€â”€ SDNET2018/             # Dataset
```

---

## ğŸ“¦ Output

Nach Training:

```
outputs/
â””â”€â”€ checkpoints/crack_encoder_TIMESTAMP/
    â”œâ”€â”€ best_model.pt          # Komplettes Model
    â””â”€â”€ encoder_weights.pt     # Nur Encoder â­
```

### Encoder Weights verwenden

```python
import torch
from torchvision.models import efficientnet_b0

# Laden
checkpoint = torch.load('encoder_weights.pt')
encoder = efficientnet_b0(weights=None)
encoder.load_state_dict(checkpoint['encoder_state_dict'])

# FÃ¼r Segmentierung verwenden
model = UNet(encoder=encoder)

# Fine-tuning Strategy:
# 1. Freeze encoder (3-5 epochs, lr=1e-3)
# 2. Unfreeze & fine-tune (10+ epochs, lr=1e-4)
```

---

## ğŸ“ˆ Erwartete Performance

### Mac (20 Epochen, 256px)

- Val AUC: 0.94-0.96
- Test AUC: 0.93-0.95
- Zeit: 5-10h

### Server (30 Epochen, 512px)

- Val AUC: 0.96-0.98
- Test AUC: 0.95-0.97
- Zeit: 2-4h
- GPU: 40% VRAM

---

## ğŸ”§ Config Anpassen

### GPU Auslastung Ã¤ndern (Server)

**Weniger VRAM (20%):**
```yaml
training:
  batch_size: 16
```

**Mehr VRAM (60%):**
```yaml
training:
  batch_size: 48
```

### Training beschleunigen

**Mac:**
```yaml
training:
  epochs: 15

augmentation:
  train:
    blur: 0.0
```

**Server:**
```yaml
training:
  batch_size: 48

data:
  num_workers: 12
```

---

## ğŸ› Troubleshooting

### CUDA Out of Memory

```yaml
training:
  batch_size: 16  # Reduzieren
```

### Training zu langsam (Mac)

```yaml
experiment:
  device: "cpu"  # Statt mps

training:
  batch_size: 64
```

### GPU Auslastung > 50%

```yaml
training:
  batch_size: 24  # Statt 32
```

### Val AUC < 0.95

```yaml
training:
  epochs: 30
  optimizer:
    lr: 2.0e-4

data:
  input_size: 640  # Server only
```

---

## ğŸ“š Dokumentation

- **`TRAINING_GUIDE.md`** - Detaillierte Training Anleitung
- **`PROJECT_STRUCTURE.md`** - Projekt Ãœbersicht
- **`data_audit.md`** - Dataset Analyse
- **`configs/*.yaml`** - Config Parameter

---

## ğŸ¯ Features

âœ… **Unified Training Pipeline** - Ein Script fÃ¼r Mac & Server
âœ… **YAML Configs** - Einfache Anpassung ohne Code
âœ… **Grouped Splitting** - Verhindert Data Leakage
âœ… **Class Balancing** - Weighted Sampling + Pos Weight
âœ… **Domain Metrics** - Per-Domain AUC/F1 Tracking
âœ… **Mixed Precision** - FP16 Training auf Server
âœ… **Early Stopping** - Automatischer Stop bei Plateau
âœ… **Encoder Export** - Direkt fÃ¼r Transfer Learning

---

## ğŸ”¬ Technische Details

**Model:**
- Backbone: EfficientNet-B0 (ImageNet)
- Parameters: ~4M
- Loss: BCEWithLogitsLoss (pos_weight=5.67)

**Optimizer:**
- AdamW (lr=3e-4, wd=1e-4)
- Cosine Annealing + Warmup
- Gradient Clipping: 1.0

**Augmentation:**
- Geometric: Flip, Rotation, Perspective
- Photometric: Brightness, Contrast, Noise, Blur
- Advanced (Server): Grid/Optical Distortion

**Data:**
- Grouped K-Fold Split (prevents leakage)
- WeightedRandomSampler (1:1 balance)
- ImageNet Normalization

---

## ğŸ“Š Monitoring

### Live Monitoring

```bash
# GPU Stats (Server)
watch -n 1 nvidia-smi

# Logs anschauen
tail -f outputs/logs/crack_encoder_*/training.log
```

### Metriken prÃ¼fen

```bash
cat outputs/logs/crack_encoder_*/test_metrics.json
```

---

## ğŸš¦ Success Criteria

Training erfolgreich wenn:

- âœ… Val AUC â‰¥ 0.95
- âœ… Test AUC â‰¥ 0.95
- âœ… Bridge AUC â‰¥ 0.90
- âœ… Pavement AUC â‰¥ 0.90
- âœ… Wall AUC â‰¥ 0.90

---

## ğŸ’¡ Tipps

### Mac
- Nachts laufen lassen
- `caffeinate -i ./scripts/train_mac.sh`
- Power Nap deaktivieren

### Server
- `tmux` oder `screen` verwenden
- GPU Temperature monitoren
- Andere User informieren (40% Auslastung)

---

## ğŸ“¦ Requirements

```bash
pip install -r requirements.txt
```

**Minimal:**
- torch>=2.0.0
- torchvision>=0.15.0
- albumentations>=1.3.0
- scikit-learn>=1.3.0
- pyyaml

**Optional:**
- tensorboard
- wandb

---

## ğŸ¤ Transfer Learning Workflow

```
1. Pre-training (SDNET2018 - Crack Detection)
   â†“
2. Encoder Export (encoder_weights.pt) â­
   â†“
3. Segmentation Model Init (U-Net/DeepLabv3+)
   â†“
4. Fine-tuning (Weld Seam Dataset)
   - Phase 1: Freeze encoder, train decoder (3-5 epochs, lr=1e-3)
   - Phase 2: Unfreeze, fine-tune all (10+ epochs, lr=1e-4)
```

---

## ğŸ“ Citation

**Dataset:**
Dorafshan, S., Thomas, R. J., & Maguire, M. (2018). SDNET2018: An annotated image dataset for training deep learning algorithms. Data in Brief, 21, 102623.

**Model:**
Tan, M., & Le, Q. (2019). EfficientNet: Rethinking model scaling for convolutional neural networks. ICML.

---

## ğŸ“ Support

Issues? Check:
1. **TRAINING_GUIDE.md** - Troubleshooting
2. **PROJECT_STRUCTURE.md** - Datei-Ãœbersicht
3. **configs/*.yaml** - Parameter Docs

---

**Version:** 2.0 (Neue Struktur)
**Erstellt:** 2025-10-27
**Status:** Production Ready ğŸš€
