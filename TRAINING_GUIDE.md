# üöÄ Training Guide

## Schnellstart

### Mac (CPU)

```bash
# 1. Dependencies installieren
pip install -r requirements.txt

# 2. Training starten
./scripts/train_mac.sh

# Oder manuell:
python src/train.py --config configs/config_mac.yaml
```

**Dauer:** ~5-10 Stunden
**Output:** `outputs/checkpoints/.../encoder_weights.pt`

---

### Server (NVIDIA A40)

```bash
# 1. Dependencies installieren
pip install -r requirements.txt

# 2. CUDA Check
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# 3. Training starten
./scripts/train_server.sh

# Oder manuell:
python src/train.py --config configs/config_server_a40.yaml
```

**Dauer:** ~2-4 Stunden
**GPU Auslastung:** ~40% VRAM (18GB von 46GB)
**Output:** `outputs/checkpoints/.../encoder_weights.pt`

---

## üìä Training √ºberwachen

### Live Monitoring (w√§hrend Training)

**Terminal 1 (Training):**
```bash
python src/train.py --config configs/config_mac.yaml
```

**Terminal 2 (Monitoring):**
```bash
# GPU Stats (Server)
watch -n 1 nvidia-smi

# CPU Stats (Mac)
top -pid $(pgrep -f train.py)
```

### Logs checken

```bash
# Neueste Logs anzeigen
tail -f outputs/logs/crack_encoder_*/training.log

# Metriken anschauen
cat outputs/logs/crack_encoder_*/test_metrics.json | python -m json.tool
```

---

## üîß Config Anpassungen

### GPU Auslastung √§ndern

**Aktuelle Auslastung:** ~40% VRAM auf A40 (18GB von 46GB)

**Weniger VRAM (20% - 9GB):**

```yaml
# configs/config_server_a40.yaml
training:
  batch_size: 16  # Statt 32
```

**Mehr VRAM (60% - 28GB):**

```yaml
training:
  batch_size: 48  # Statt 32
```

**VRAM Berechnung:**
- Batch 16 @ 512px: ~9GB (20%)
- Batch 32 @ 512px: ~18GB (40%) ‚Üê **Standard**
- Batch 48 @ 512px: ~27GB (60%)
- Batch 64 @ 512px: ~36GB (78%)

### Training beschleunigen

**Mac (CPU):**

```yaml
# Weniger Augmentations
augmentation:
  train:
    brightness_contrast: 0.3  # Statt 0.5
    blur: 0.0                  # Deaktivieren

# Weniger Epochen
training:
  epochs: 15  # Statt 20
```

**Server (GPU):**

```yaml
# Gr√∂√üere Batch Size
training:
  batch_size: 48  # Statt 32

# Mehr Workers
data:
  num_workers: 12  # Statt 8

# Benchmark Mode
hardware:
  benchmark: true
  deterministic: false
```

### Accuracy verbessern

```yaml
# H√∂here Aufl√∂sung
data:
  input_size: 640  # Statt 512 (Server only!)

# Mehr Epochen
training:
  epochs: 40  # Statt 30

# Mehr Augmentation
augmentation:
  train:
    horizontal_flip: 0.7
    vertical_flip: 0.3
    rotate_prob: 0.5
```

---

## üìà Erwartete Performance

### Mac Training (20 Epochen, 256px)

| Metrik | Erwartung |
|--------|-----------|
| Val AUC | 0.94-0.96 |
| Test AUC | 0.93-0.95 |
| Test F1 | 0.85-0.90 |
| Training Zeit | 5-10h |

### Server Training (30 Epochen, 512px)

| Metrik | Erwartung |
|--------|-----------|
| Val AUC | 0.96-0.98 |
| Test AUC | 0.95-0.97 |
| Test F1 | 0.88-0.92 |
| Training Zeit | 2-4h |
| GPU VRAM | ~18GB |
| GPU Util | 40-60% |

---

## üéØ Success Criteria

Training ist erfolgreich wenn:

‚úÖ **Val AUC ‚â• 0.95**
‚úÖ **Test AUC ‚â• 0.95**
‚úÖ **Alle Domain AUCs ‚â• 0.90**
- Bridge AUC ‚â• 0.90
- Pavement AUC ‚â• 0.90
- Wall AUC ‚â• 0.90

---

## üì¶ Outputs verwenden

### Encoder Weights laden

```python
import torch
from torchvision.models import efficientnet_b0

# Laden
checkpoint = torch.load('outputs/checkpoints/.../encoder_weights.pt')
encoder = efficientnet_b0(weights=None)
encoder.load_state_dict(checkpoint['encoder_state_dict'])

# Info
config = checkpoint['config']
print(f"Pretrained on: {config['pretrained_on']}")
print(f"Input size: {config['input_size']}px")
print(f"Best Val AUC: {config['best_val_auc']:.4f}")
print(f"Normalization: mean={config['mean']}, std={config['std']}")
```

### F√ºr Segmentierung verwenden

```python
# U-Net Beispiel
class UNet(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

        # Decoder bauen
        self.decoder = nn.Sequential(
            # ... Decoder Layers
        )

    def forward(self, x):
        # Encoder Features
        features = self.encoder(x)

        # Decoder
        output = self.decoder(features)
        return output

# Initialisieren mit pre-trained Encoder
encoder = load_encoder('encoder_weights.pt')
model = UNet(encoder)

# Fine-tuning Strategy:
# 1. Freeze encoder (3-5 epochs)
for param in encoder.parameters():
    param.requires_grad = False

# Train decoder only
optimizer = torch.optim.AdamW(model.decoder.parameters(), lr=1e-3)

# 2. Unfreeze and fine-tune (10+ epochs)
for param in encoder.parameters():
    param.requires_grad = True

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
```

---

## üêõ Troubleshooting

### Training startet nicht

**Problem:** Import Fehler
```
ModuleNotFoundError: No module named 'albumentations'
```

**L√∂sung:**
```bash
pip install -r requirements.txt
```

---

### CUDA Out of Memory (Server)

**Problem:**
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**L√∂sung 1:** Batch Size reduzieren
```yaml
training:
  batch_size: 16  # Statt 32
```

**L√∂sung 2:** Gradient Accumulation erh√∂hen
```yaml
performance:
  gradient_accumulation_steps: 4  # Statt 2
```

**L√∂sung 3:** Input Size reduzieren
```yaml
data:
  input_size: 384  # Statt 512
```

---

### Training zu langsam (Mac)

**Problem:** MPS langsamer als erwartet

**L√∂sung:** CPU verwenden
```yaml
experiment:
  device: "cpu"  # Statt mps

training:
  batch_size: 64  # Gr√∂√üer f√ºr CPU
```

---

### Validation AUC < 0.95

**Problem:** Performance zu niedrig

**L√∂sung 1:** Mehr Epochen
```yaml
training:
  epochs: 30  # Statt 20
```

**L√∂sung 2:** Learning Rate anpassen
```yaml
training:
  optimizer:
    lr: 2.0e-4  # Statt 3.0e-4 (kleiner = stabiler)
```

**L√∂sung 3:** Mehr Augmentation
```yaml
augmentation:
  train:
    horizontal_flip: 0.7
    brightness_contrast: 0.6
```

**L√∂sung 4:** H√∂here Aufl√∂sung (Server)
```yaml
data:
  input_size: 640  # Statt 512
```

---

### GPU Auslastung zu hoch (>50%)

**Problem:** Server soll nur 40% nutzen

**L√∂sung:**
```yaml
training:
  batch_size: 24  # Statt 32 (von 40% auf ~30%)
```

Oder:

```yaml
training:
  batch_size: 20  # Noch weniger (von 40% auf ~25%)
```

**Monitoring:**
```bash
watch -n 1 'nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used --format=csv,noheader'
```

---

## üìä Monitoring Tools

### TensorBoard (Optional)

**1. Config aktivieren:**
```yaml
monitoring:
  use_tensorboard: true
```

**2. TensorBoard starten:**
```bash
tensorboard --logdir outputs/logs
```

**3. Browser √∂ffnen:**
```
http://localhost:6006
```

### Weights & Biases (Optional)

**1. Config aktivieren:**
```yaml
monitoring:
  use_wandb: true
  project_name: "crack-detection"
```

**2. Login:**
```bash
wandb login
```

**3. Training startet automatisch Tracking**

---

## üîÑ Resume Training

Falls Training abbricht:

**1. Checkpoint pr√ºfen:**
```bash
ls -lh outputs/checkpoints/crack_encoder_*/best_model.pt
```

**2. Training fortsetzen:**
```python
# TODO: Resume Funktion implementieren
# Aktuell: Training startet neu
```

---

## üìù Tipps & Best Practices

### Mac Training

‚úÖ **Empfehlungen:**
- Nachts laufen lassen (5-10h)
- Power Nap deaktivieren
- Terminal offen lassen
- `caffeinate` verwenden:
  ```bash
  caffeinate -i ./scripts/train_mac.sh
  ```

‚ùå **Vermeiden:**
- Parallele CPU-intensive Tasks
- Safari mit vielen Tabs
- Video Encoding w√§hrend Training

### Server Training

‚úÖ **Empfehlungen:**
- `tmux` oder `screen` verwenden
- GPU Temperature monitoren
- Logs regelm√§√üig checken
- Andere User informieren (40% Auslastung)

‚ùå **Vermeiden:**
- Batch Size > 64 (VRAM Limit)
- Mehrere Trainings parallel
- GPU ohne Monitoring

### Allgemein

‚úÖ **Best Practices:**
- Config committen (Git)
- Outputs regelm√§√üig sichern
- Metriken dokumentieren
- Encoder Weights umbenennen mit AUC:
  ```bash
  mv encoder_weights.pt encoder_auc_0.9678.pt
  ```

---

## üìö Weitere Ressourcen

- **PROJECT_STRUCTURE.md** - Projekt √úbersicht
- **README.md** - Hauptdokumentation
- **data_audit.md** - Dataset Analyse
- **configs/config_*.yaml** - Config Parameter

---

**Viel Erfolg beim Training! üöÄ**
