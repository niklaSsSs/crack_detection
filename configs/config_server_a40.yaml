# Configuration for NVIDIA A40 Server Training
# Optimiert für ~40% GPU Auslastung (max 18GB VRAM von 46GB)

experiment:
  name: "crack_encoder_a40"
  seed: 42
  device: "cuda"
  mixed_precision: true  # FP16 für schnelleres Training

data:
  data_dir: "./SDNET2018"  # Anpassen an Server-Pfad
  input_size: 512  # Höhere Auflösung für bessere Features
  num_workers: 8  # Server hat mehr Cores
  pin_memory: true  # Schneller mit CUDA

  # Split ratios
  train_ratio: 0.8
  val_ratio: 0.1
  test_ratio: 0.1

  # Normalization (ImageNet)
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

model:
  backbone: "efficientnet_b0"
  pretrained: true
  num_classes: 1  # Binary classification
  dropout: 0.2

training:
  # Batch size für ~40% GPU Auslastung
  # A40: 46GB VRAM, Ziel: ~18GB = 40%
  # EfficientNet-B0 @ 512x512: ~0.6GB pro Batch von 32
  # → 32 samples × 0.6GB = ~19GB (mit Overhead)
  batch_size: 32  # Konservativ für stabile 40% Auslastung

  epochs: 30  # Mehr Epochen für bessere Konvergenz

  optimizer:
    name: "adamw"
    lr: 3.0e-4
    weight_decay: 1.0e-4
    betas: [0.9, 0.999]

  scheduler:
    name: "cosine"
    warmup_epochs: 2
    min_lr: 1.0e-6

  loss:
    name: "bce_with_logits"
    pos_weight_auto: true

  early_stopping:
    patience: 7  # Mehr Patience für 30 Epochen
    min_delta: 0.0005
    metric: "val_auc"
    mode: "max"

  # Gradient clipping
  grad_clip: 1.0

  # Checkpoint
  save_top_k: 5
  save_last: true

augmentation:
  train:
    # Geometrisch
    horizontal_flip: 0.5
    vertical_flip: 0.2  # Zusätzlich für Server
    rotate_limit: 10
    rotate_prob: 0.4
    shift_scale_rotate: 0.3
    perspective: 0.2

    # Photometrisch (aggressiver)
    brightness_contrast: 0.5
    hue_saturation: 0.4
    random_gamma: 0.3
    gauss_noise: 0.3
    gauss_blur: 0.3
    motion_blur: 0.2
    image_compression: 0.3

    # Erweitert
    grid_distortion: 0.2
    optical_distortion: 0.2

  val:
    # Nur Resize + Normalize
    enabled: false

logging:
  log_dir: "./outputs/logs"
  checkpoint_dir: "./outputs/checkpoints"
  visualization_dir: "./outputs/visualizations"

  log_every_n_steps: 25  # Häufiger loggen
  save_plots: true

  metrics:
    - "loss"
    - "auc"
    - "f1"
    - "precision"
    - "recall"

  domain_metrics: true

  # Zusätzlich für Server
  log_gpu_stats: true
  log_learning_rate: true

hardware:
  # GPU Settings für A40
  gpu_id: 0
  deterministic: false  # Schneller, aber weniger reproduzierbar
  benchmark: true  # cuDNN Auto-Tuning für Speed

  # Multi-GPU (optional, aktuell 1 GPU)
  distributed: false
  world_size: 1

  # Memory Management
  empty_cache_every_n_steps: 100

# Server-spezifische Optimierungen
performance:
  # Gradient Accumulation für effektiv größere Batch Size
  # Effektive Batch Size = batch_size × accumulation_steps
  gradient_accumulation_steps: 2  # Effektiv 64 samples

  # Async Data Loading
  prefetch_factor: 2  # Vorladen von 2 Batches
  persistent_workers: true  # Worker nicht neu starten

  # Compile (PyTorch 2.0+)
  compile_model: false  # Experimentell, kann Fehler geben

# Monitoring
monitoring:
  # Weights & Biases / TensorBoard
  use_wandb: false
  use_tensorboard: true

  project_name: "sdnet2018-encoder"
  tags: ["a40", "efficientnet_b0", "crack_detection"]
